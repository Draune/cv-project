{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4900755b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CV Starter — Colab/Kaggle Quickstart\n",
    "\n",
    "> Author : Badr TAJINI\n",
    "\n",
    "**Academic year:** 2025–2026  \n",
    "**School:** ECE  \n",
    "**Course:** Machine Learning & Deep Learning 2 \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Welcome! This notebook is intentionally beginner-friendly. Follow the steps exactly and you will confirm that the starter project works on a free GPU runtime.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16000ed3-81d5-42dd-a401-75d57493b2aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## M0: Environment Setup\n",
    "\n",
    "Nous allons utiliser **Colab** pour entrainer notre modèle, cette section décrit comment mettre en place le projet pour l'exécuter dans cet environment.\n",
    "\n",
    "### Before you run anything\n",
    "\n",
    "1. **Open the notebook in Google Colab or Kaggle.**\n",
    "2. **Change the hardware accelerator to GPU (T4 preferred): Runtime` → `Change runtime type` → Hardware accelerator `GPU` → Save.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1b9df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Project files\n",
    "\n",
    "Ouvrez le terminal Colab et entrez cette commande pour télécharger le projet:\n",
    "`git clone https://github.com/Draune/cv-project cv-project`\n",
    "\n",
    "Vous devriez maintenant avoir le dossier `/content/cv-project`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b435543",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Confirm the GPU is ready\n",
    "\n",
    "La cellule suivante nous permet de vérifier que le GPU est bien actif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e4189",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi || echo \"nvidia-smi unavailable (CPU runtime)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72df6f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Point the notebook at the project folder\n",
    "\n",
    "Cette cellule permet de s'assurer que le projet est bien présent dans `/content/cv-project`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee2351",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent.resolve()\n",
    "elif PROJECT_ROOT.name == \"content\":\n",
    "    candidate = PROJECT_ROOT / \"cv-project\"\n",
    "    if candidate.exists():\n",
    "        PROJECT_ROOT = candidate.resolve()\n",
    "\n",
    "if not (PROJECT_ROOT / \"src\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not locate project root at {PROJECT_ROOT}. Clone or upload cv-project before proceeding.\"\n",
    "    )\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT / \"src\") not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT / \"src\"))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258db80c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Install the project requirements\n",
    "\n",
    "Ensuite nous installons les dépendances du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed260df",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install project dependencies listed in requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500e89f-dff5-45c5-8971-d252f2108db1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## M1: Problem Scoping & Data Validation\n",
    "\n",
    "Notre dataset sera le CIFAR-10, il contient 60 000 images couleurs de dimension 32x32 réparties en 10 classes différentes.\n",
    "\n",
    "Les 10 classes du dataset sont : airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, et trucks.\n",
    "\n",
    "Il s'agit d'un dataset créé par l'université de Toronto sous licence MIT (libre pour l’usage éducatif et commercial).\n",
    "\n",
    "Les potentiels biais sont les suivants :\n",
    "\n",
    "- Les images sont très petites, on a donc un perte d'information visuelle\n",
    "- Les images sont peu représentative de la réalité (images propres, peu de bruit, faible résolution)\n",
    "\n",
    "Cependant le format \"petit\" du dataset permet de tester rapidement différents algorithmes, est parfait pour l'usage éducatif et permet de comparer ses résultats avec d'autres recherches car il est très utilisé.\n",
    "\n",
    "Le dataset est déja divisé en train/validation subset avec 50 000 images pour l'entrainement et 10 000 pour la validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1d966-5d09-46b5-8bbe-4d098f6a56ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## M2: Baseline Model Implementation\n",
    "\n",
    "Nous allons utiliser l'implémentation fournie par `CV starter` donc nous allons utiliser le modèle ResNet-18.\n",
    "\n",
    "Ainsi, nous allons utiliser le fichier train.py afin d'entrainer notre modèle et evaluate.py pour analyser les résultats de l'entrainement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4562c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Run the smoke test\n",
    "\n",
    "Le smoke test permet de s'assurer que le model fonctionne bien en ne réalisant qu'une seule epoch et sauvegarde ses résultats dans `outputs/smoke_metrics.json`.\n",
    "\n",
    "L'output de la cellule dois être un court JSON comme `{\"loss\": ..., \"batch_size\": 64, ...}`.\n",
    "\n",
    "Ainsi nous pouvons nous assurer que :\n",
    "- l'on peut télécharger et lire le dataset, \n",
    "- le modèle s'exécute correctement, \n",
    "- les dépendances sont bien installées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa6b73",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src import smoke_check\n",
    "\n",
    "smoke_path = smoke_check.run_smoke(\"configs/cv_cifar10_fast.yaml\")\n",
    "print(smoke_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f22207e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training to establish a performance baseline\n",
    "\n",
    "L'entrainement se déroule suivant les configuration YAML dans le dossier `configs` (ex. `configs/cv_cifar10_fast.yaml` pour le smoke test). Pour la baseline nous allons utilisé la configuration `configs/cv_cifar10_baseline.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a6f4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python src/train.py --config configs/cv_cifar10_fast.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484af641",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python src/evaluate.py --config configs/cv_cifar10.yaml --ckpt outputs/best.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d4d59a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 4 — What should I see now?\n",
    "- `outputs/best.pt`: saved checkpoint.\n",
    "- `outputs/log.csv`: training history (loss/accuracy per epoch).\n",
    "- `outputs/eval.json`, `per_class_metrics.csv`, `confusion_matrix.png`, `leaderboard.png`: evaluation artefacts.\n",
    "\n",
    "If any of these files are missing, scroll up for errors in the training/evaluation cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da9cf1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Mirror this workflow for other tracks\n",
    "\n",
    "1. Duplicate this notebook and rename it (e.g., `00_nlp_quickstart.ipynb`).\n",
    "2. Copy the corresponding starter folder into your Colab/Kaggle workspace (`nlp-project`, `od-project`, `ts-project`).\n",
    "3. Update the install cell so it matches that starter's `requirements.txt`.\n",
    "4. Replace `from src import smoke_check` with the helper module provided in the new starter (each repo ships with one).\n",
    "5. Point the train/eval commands at the new `configs/*.yaml` file.\n",
    "6. Optionally tweak the markdown text so instructions mention the right dataset and metrics.\n",
    "\n",
    "By following the same structure—GPU check → install → smoke test → full run—students can master all four tracks with a consistent workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "name": "01_colab_final.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
